---
theme: default
class: text-center
highlighter: shiki
lineNumbers: true
info: |
  ## Forutsigbar agentisk oppfÃ¸rsel med LangChain og MLflow
  
  Magnus RÃ¸dseth og HÃ¥vard Opheim â€” Capra Consulting @ Gjensidige
drawings:
  persist: false
transition: slide-left
title: Forutsigbar agentisk oppfÃ¸rsel med LangChain og MLflow
mdc: true
---

# Forutsigbar agentisk oppfÃ¸rsel med LangChain og MLflow

Magnus RÃ¸dseth og HÃ¥vard Opheim

Capra Consulting @ Gjensidige

<div class="abs-br m-6 flex gap-8 items-center bg-white rounded-lg px-4 py-2">
  <img src="/capra-logo.png" class="h-5" alt="Capra Consulting" />
  <img src="/gjensidige-logo.png" class="h-6" alt="Gjensidige" />
</div>

<!--
Velkommen! Vi er Magnus og HÃ¥vard fra Capra Consulting.
I dag skal vi vise hvordan vi bruker LangChain og MLflow for Ã¥ bygge forutsigbare AI-agenter hos Gjensidige.
Presentasjonen varer 20 minutter.
-->

---
layout: two-cols
---

# Hvem er vi?

<v-clicks>

- **Magnus RÃ¸dseth** og **HÃ¥vard Opheim**
- Utviklere i Capra Consulting
- Jobber hos Gjensidige â€” Norges stÃ¸rste forsikringsselskap
- Bygger AI-drevne lÃ¸sninger for boligeiere

</v-clicks>

::right::

<v-click>

## Dagens tema

**Forutsigbar agentisk oppfÃ¸rsel med LangChain og MLflow**

<div class="mt-8">

Agentiske applikasjoner kan oppleves som **uforutsigbare** og **vanskelig Ã¥ forholde seg til** som utviklere.

Vi viser hvordan vi bruker **MLflow** og **LangChain** for Ã¥ observere og guide adferden.

</div>

</v-click>

<div class="abs-br m-6 flex gap-8 items-center bg-white rounded-lg px-4 py-2">
  <img src="/capra-logo.png" class="h-5" alt="Capra Consulting" />
  <img src="/gjensidige-logo.png" class="h-6" alt="Gjensidige" />
</div>

<!--
Hei alle sammen. Vi heter Magnus RÃ¸dseth og HÃ¥vard Opheim, og vi er utviklere i Capra Consulting.
Vi jobber for tiden pÃ¥ et nytt prosjekt hos Gjensidige â€” Norges stÃ¸rste forsikringsselskap.

Temaet for denne presentasjonen er: Forutsigbar agentisk oppfÃ¸rsel med LangChain og MLflow.
Agentiske applikasjoner og store sprÃ¥kmodeller kan ofte oppleves som uforutsigbare og vanskelig Ã¥ forholde seg til som utviklere.
Vi skal vise dere hvordan vi bruker MLflow og LangChain hos Gjensidige for Ã¥ observere og guide adferden i agentiske applikasjoner.

Men fÃ¸rst â€” en rask introduksjon til verktÃ¸yene vi bruker, for de som ikke har jobbet med dette fÃ¸r.
-->

---
layout: section
class: text-center
---

## Lynkurs: LangChain, agenter og samtaledesign

<!--
FÃ¸r vi dykker inn i produktet, la oss sÃ¸rge for at alle er pÃ¥ samme side.
-->

---

# Hva er LangChain?

<div class="grid grid-cols-3 gap-4 mt-8">

<v-click>
<div class="border rounded p-4 text-center">
  <div class="text-4xl mb-2">ğŸ“</div>
  <div class="font-bold">Prompts</div>
  <div class="text-sm opacity-75">Prompt-maler</div>
</div>
</v-click>

<v-click>
<div class="border rounded p-4 text-center">
  <div class="text-4xl mb-2">ğŸ¤–</div>
  <div class="font-bold">Modell</div>
  <div class="text-sm opacity-75">LLM-abstraksjoner</div>
</div>
</v-click>

<v-click>
<div class="border rounded p-4 text-center">
  <div class="text-4xl mb-2">âœ¨</div>
  <div class="font-bold">Output</div>
  <div class="text-sm opacity-75">Strukturert respons</div>
</div>
</v-click>

</div>

<v-click>

<div class="mt-8">

**LangChain** er et Python-bibliotek som gjÃ¸r det enklere Ã¥ bygge applikasjoner med store sprÃ¥kmodeller.

I stedet for Ã¥ skrive rÃ¥ API-kall mot OpenAI, gir LangChain deg byggeblokker: prompt-maler, modellabstraksjoner, verktÃ¸y og minnehÃ¥ndtering.

</div>

</v-click>

<v-click>

<div class="mt-4 p-4 bg-blue-500 bg-opacity-10 rounded">

**LangGraph** er et sÃ¸sterbibliotek som lar deg bygge _agenter_ â€” LLM-applikasjoner som kan ta flere steg, bruke verktÃ¸y, og ta avgjÃ¸relser underveis.

</div>

</v-click>

<!--
LangChain er et Python-bibliotek som gjÃ¸r det enklere Ã¥ bygge applikasjoner med store sprÃ¥kmodeller.
I stedet for Ã¥ skrive rÃ¥ API-kall mot OpenAI, gir LangChain deg byggeblokker: prompt-maler, modellabstraksjoner, verktÃ¸y og minnehÃ¥ndtering.
Tenk pÃ¥ det som et rammeverk som lar deg sette sammen LLM-baserte pipelines uten Ã¥ finne opp hjulet hver gang.

LangGraph er et sÃ¸sterbibliotek som lar deg bygge agenter â€” altsÃ¥ LLM-applikasjoner som kan ta flere steg, bruke verktÃ¸y, og ta avgjÃ¸relser underveis.
Du definerer flyten som en graf med noder og kanter, og LangGraph hÃ¥ndterer tilstand, feilhÃ¥ndtering og parallellitet.
-->

---

# Hva er en agent?

<div class="grid grid-cols-2 gap-8 mt-8">

<div>

## Vanlig LLM-integrasjon

```
Bruker â†’ SpÃ¸rsmÃ¥l
   â†“
Modell â†’ Svar
```

<div class="text-center mt-4 opacity-75">
Ã‰n runde. Ferdig.
</div>

</div>

<div>

## Agent

<v-clicks>

- ğŸ§  **Resonnere** over spÃ¸rsmÃ¥let og planlegge neste steg
- ğŸ› ï¸ **Bruke verktÃ¸y** â€” hente data, sÃ¸ke i dokumenter, kalle API-er
- ğŸ”„ **Iterere** â€” sjekke svaret sitt, prÃ¸ve pÃ¥ nytt
- ğŸ¤ **Delegere** â€” sende deloppgaver til andre agenter

</v-clicks>

</div>

</div>

<v-click>

<div class="mt-8 p-4 bg-green-500 bg-opacity-10 rounded">

**I praksis:** Du gir en LLM tilgang til verktÃ¸y og lar den selv bestemme _om_ og _nÃ¥r_ den skal bruke dem.

</div>

</v-click>

<!--
En vanlig LLM-integrasjon er: bruker sender spÃ¸rsmÃ¥l â†’ modell svarer. Ã‰n runde.

En agent er annerledes â€” den kan:
- Resonnere over spÃ¸rsmÃ¥let og planlegge neste steg
- Bruke verktÃ¸y â€” hente data fra en database, sÃ¸ke i dokumenter, kalle API-er
- Iterere â€” sjekke svaret sitt, og prÃ¸ve pÃ¥ nytt hvis det ikke er godt nok
- Delegere â€” sende deloppgaver til andre agenter

I praksis betyr det at du gir en LLM tilgang til verktÃ¸y og lar den selv bestemme om og nÃ¥r den skal bruke dem.
-->

---

# Tre nÃ¸kkelprinsipper for agentisk samtaledesign

<v-clicks>

<div class="mt-8">

## 1. Intensjonsklassifisering

Hva prÃ¸ver brukeren Ã¥ oppnÃ¥? Vi klassifiserer hvert spÃ¸rsmÃ¥l for Ã¥ velge riktig behandling.

</div>

<div class="mt-8">

## 2. Kontekstinjeksjon

Vi henter relevant informasjon (fra dokumenter, databaser) og gir det til modellen _fÃ¸r_ den svarer.

</div>

<div class="mt-8">

## 3. Guardrails

Vi definerer hva agenten _ikke_ skal svare pÃ¥. Holdningen er: **"GjÃ¸r Ã©n ting, gjÃ¸r den bra."**

</div>

</v-clicks>

<!--
NÃ¥r du designer samtaler for agenter, er det tre nÃ¸kkelprinsipper:

1. Intensjonsklassifisering â€” Hva prÃ¸ver brukeren Ã¥ oppnÃ¥? Vi klassifiserer hvert spÃ¸rsmÃ¥l for Ã¥ velge riktig behandling.

2. Kontekstinjeksjon â€” Vi henter relevant informasjon fra dokumenter og databaser og gir det til modellen fÃ¸r den svarer.

3. Guardrails â€” Vi definerer hva agenten ikke skal svare pÃ¥. Holdningen er: "GjÃ¸r Ã©n ting, gjÃ¸r den bra."
-->

---

# Hvordan koble pÃ¥ LangChain â€” i tre steg

````md magic-move
```python
# 1. Lag en modell
from langchain_openai import AzureChatOpenAI
model = AzureChatOpenAI(model="gpt-4.1")
```

```python
# 1. Lag en modell
from langchain_openai import AzureChatOpenAI
model = AzureChatOpenAI(model="gpt-4.1")

# 2. Lag en prompt
from langchain_core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_messages([
    ("system", "Du er en boligrÃ¥dgiver."),
    ("human", "{question}"),
])
```

```python
# 1. Lag en modell
from langchain_openai import AzureChatOpenAI
model = AzureChatOpenAI(model="gpt-4.1")

# 2. Lag en prompt
from langchain_core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_messages([
    ("system", "Du er en boligrÃ¥dgiver."),
    ("human", "{question}"),
])

# 3. Koble sammen og kjÃ¸r
chain = prompt | model
response = chain.invoke({"question": "Hva betyr TG2?"})
```
````

<!--
Det fine med LangChain er at terskelen er lav. I sin enkleste form:

1. Lag en modell â€” her bruker vi Azure OpenAI
2. Lag en prompt â€” en mal med system-melding og bruker-spÃ¸rsmÃ¥l
3. Koble sammen og kjÃ¸r â€” bruk pipe-operatoren til Ã¥ lage en chain

Tre linjer, og du har en fungerende LLM-pipeline.
Derfra bygger du pÃ¥ med verktÃ¸y, agentlogikk, RAG, og â€” som vi skal se â€” observabilitet.
-->

---

# Hva bÃ¸r man se etter og benchmarke?

<div class="grid grid-cols-3 gap-3 mt-6">

<v-click>
<div class="border rounded p-3">
  <div class="text-xl mb-1">â±ï¸</div>
  <div class="font-bold text-sm">Latency</div>
  <div class="text-xs opacity-75">Hvor lang tid tar hvert steg?</div>
</div>
</v-click>

<v-click>
<div class="border rounded p-3">
  <div class="text-xl mb-1">ğŸ­</div>
  <div class="font-bold text-sm">Hallusinering</div>
  <div class="text-xs opacity-75">Finner modellen opp ting?</div>
</div>
</v-click>

<v-click>
<div class="border rounded p-3">
  <div class="text-xl mb-1">ğŸ› ï¸</div>
  <div class="font-bold text-sm">VerktÃ¸ybruk</div>
  <div class="text-xs opacity-75">Valgte agenten riktig verktÃ¸y?</div>
</div>
</v-click>

<v-click>
<div class="border rounded p-3">
  <div class="text-xl mb-1">ğŸ¯</div>
  <div class="font-bold text-sm">Kontekstrelevans</div>
  <div class="text-xs opacity-75">Hentet RAG riktige dokumenter?</div>
</div>
</v-click>

<v-click>
<div class="border rounded p-3">
  <div class="text-xl mb-1">ğŸ“‰</div>
  <div class="font-bold text-sm">Regressjoner</div>
  <div class="text-xs opacity-75">DÃ¥rligere etter siste endring?</div>
</div>
</v-click>

</div>

<v-click>

<div class="mt-6 p-4 bg-purple-500 bg-opacity-10 rounded text-center">

**Dette er nÃ¸yaktig det vi skal vise dere hvordan vi mÃ¥ler â€” med MLflow.**

</div>

</v-click>

<!--
NÃ¥r du bygger med agenter, er det noen ting du bÃ¸r fÃ¸lge med pÃ¥ fra dag Ã©n:

- Latency â€” Hvor lang tid tar hvert steg? Agenter kan bli trege fordi de tar flere runder.
- Hallusinering â€” Finner modellen opp ting? Spesielt viktig nÃ¥r den svarer basert pÃ¥ brukerens dokumenter.
- VerktÃ¸ybruk â€” Valgte agenten riktig verktÃ¸y? Kalte den det med riktige parametere?
- Kontekstrelevans â€” Hentet RAG-modulen de riktige dokumentene, eller bare de mest populÃ¦re?
- Regressjoner â€” Ble svarkvaliteten dÃ¥rligere etter siste prompt-endring?

Dette er nÃ¸yaktig det vi skal vise dere hvordan vi mÃ¥ler â€” med MLflow.
-->

---
layout: section
class: text-center
---

## Hva bygger vi?

<!--
NÃ¥ som dere har en grunnleggende forstÃ¥else av LangChain og agenter, la oss se pÃ¥ hva vi faktisk bygger.
-->

---

# Hei, huset! â€” Snakk med boligen din

<div class="grid grid-cols-2 gap-8">

<div>

<v-clicks>

**Hei, huset!** â€” en AI-assistent som lar norske boligeiere "snakke med huset sitt".

Du har nettopp kjÃ¸pt en bolig:
- Tilstandsrapport pÃ¥ 40 sider
- Salgsoppgave
- FDV-dokumentasjon

**LÃ¸sningen:** Last opp dokumentene, og spÃ¸r:
- "Hva er tilstanden pÃ¥ taket mitt?"
- "Hva bÃ¸r jeg prioritere Ã¥ fikse fÃ¸rst?"

</v-clicks>

</div>

<div class="flex items-center justify-center">
  <div class="border-2 border-dashed border-yellow-400 rounded-lg p-6 text-yellow-400 text-center w-full h-56 flex flex-col items-center justify-center">
    <div class="text-3xl mb-2">ğŸ’¬ğŸ </div>
    <div class="text-sm font-bold">ğŸ“¸ PLACEHOLDER: App-screenshot</div>
    <div class="text-xs mt-2">Screenshot av Hei, huset!-appen (chat eller dashboard)</div>
    <div class="text-xs opacity-75 mt-1">Place as <code>/public/screenshots/hei-huset-app.png</code></div>
  </div>
</div>

</div>

<!--
Vi bygger Hei, huset! â€” en AI-assistent som lar norske boligeiere "snakke med huset sitt".

Tenk deg at du nettopp har kjÃ¸pt en bolig. Du har en tilstandsrapport pÃ¥ 40 sider, en salgsoppgave, kanskje FDV-dokumentasjon.
Alt dette er viktig informasjon om boligen din, men hvem leser egentlig 40 sider med fagsprÃ¥k om tilstandsgrader og avvik?

Det er her Hei, huset! kommer inn. Du laster opp dokumentene dine, og sÃ¥ kan du bare spÃ¸rre:
"Hva er tilstanden pÃ¥ taket mitt?" eller "Hva bÃ¸r jeg prioritere Ã¥ fikse fÃ¸rst?"
-->

---

# Hva kan applikasjonen gjÃ¸re?

## 1. Chat med kontekst

<div class="border-2 border-dashed border-yellow-400 rounded-lg p-6 text-yellow-400 text-center mt-4 h-48 flex flex-col items-center justify-center">
  <div class="text-3xl mb-2">ğŸ’¬</div>
  <div class="text-sm font-bold">ğŸ“¸ PLACEHOLDER: Screenshot av chat-grensesnittet</div>
  <div class="text-xs mt-2">En samtale der brukeren spÃ¸r om taket sitt og fÃ¥r et personalisert svar med kildehenvisninger</div>
  <div class="text-xs opacity-75 mt-1">Place as <code>/public/screenshots/chat-interface.png</code></div>
</div>

<v-clicks>

- Chatte med AI-en og stille spÃ¸rsmÃ¥l om boligen din
- AI-en bruker informasjon fra dokumentene dine og tilstandsvurderinger
- Du kan "tagge" kontekst â€” for eksempel en spesifikk tilstandsvurdering eller et varsel
- Personaliserte svar med kildehenvisninger

</v-clicks>

<!--
La meg ta dere gjennom de viktigste funksjonene.

FÃ¸rst: Chat med kontekst.
Du kan chatte med AI-en og stille spÃ¸rsmÃ¥l om boligen din.
AI-en bruker informasjon fra dokumentene dine og tilstandsvurderinger for Ã¥ gi personaliserte svar.
Du kan "tagge" kontekst â€” for eksempel en spesifikk tilstandsvurdering eller et varsel â€” og AI-en tar hensyn til det i svaret.
-->

---

# 2. Dokumentanalyse med AI

<div class="border-2 border-dashed border-yellow-400 rounded-lg p-6 text-yellow-400 text-center mt-4 h-40 flex flex-col items-center justify-center">
  <div class="text-3xl mb-2">ğŸ“Š</div>
  <div class="text-sm font-bold">ğŸ“¸ PLACEHOLDER: Screenshot av property-siden</div>
  <div class="text-xs mt-2">Tilstandsvurderinger med TG-kort, fargekoder, ELI5-forklaringer, konfidensindikatorer</div>
  <div class="text-xs opacity-75 mt-1">Place as <code>/public/screenshots/property-conditions.png</code></div>
</div>

NÃ¥r du laster opp en tilstandsrapport, kjÃ¸rer vi en ekstraheringsagent som:

<v-clicks>

- ğŸ“„ Leser og tolker hver side med vision-modell (PDF â†’ Markdown)
- ğŸ” Finner alle tilstandsvurderinger (TG0â€“TG3) med kategori, beskrivelse og anbefaling
- ğŸ“Š Vurderer pÃ¥liteligheten pÃ¥ hvert funn â€” hÃ¸y, middels eller lav
- ğŸ’¡ Skriver en "explain like I'm 5"-forklaring for hvert punkt, uten faguttrykk
- â­ Prioriterer de 3â€“5 viktigste punktene du bÃ¸r ta tak i

</v-clicks>

<!--
Funksjon nummer to: Dokumentanalyse med AI.

NÃ¥r du laster opp en tilstandsrapport, kjÃ¸rer vi en ekstraheringsagent som:
- Leser og tolker hver side med vision-modell â€” PDF til Markdown
- Finner alle tilstandsvurderinger (TG0 til TG3) med kategori, beskrivelse og anbefaling
- Vurderer pÃ¥liteligheten pÃ¥ hvert funn â€” hÃ¸y, middels eller lav
- Skriver en "explain like I'm 5"-forklaring for hvert punkt, uten faguttrykk
- Prioriterer de 3 til 5 viktigste punktene du bÃ¸r ta tak i
-->

---
layout: two-cols
---

# 3. Varsler

<div class="border-2 border-dashed border-yellow-400 rounded-lg p-4 text-yellow-400 text-center mt-4 h-32 flex flex-col items-center justify-center">
  <div class="text-2xl mb-1">âš ï¸</div>
  <div class="text-xs font-bold">ğŸ“¸ PLACEHOLDER: Varsler-siden</div>
  <div class="text-xs mt-1">Frostvarsel med personaliserte tiltak</div>
  <div class="text-xs opacity-75">â†’ <code>/public/screenshots/warnings.png</code></div>
</div>

<v-click>

Gjensidige sender varsler til boligeiere:
- Frost
- Kraftig nedbÃ¸r
- Stormvarsel

Brukeren kan klikke pÃ¥ et varsel og fÃ¥ **personaliserte tiltak** for sin bolig.

</v-click>

::right::

# 4. Forbedringsplaner

<v-click>

<div class="border-2 border-dashed border-yellow-400 rounded-lg p-4 text-yellow-400 text-center mt-4 h-32 flex flex-col items-center justify-center">
  <div class="text-2xl mb-1">ğŸ“‹</div>
  <div class="text-xs font-bold">ğŸ“¸ PLACEHOLDER: Forbedringsplan</div>
  <div class="text-xs mt-1">Utbedringsplan med prioritering</div>
  <div class="text-xs opacity-75">â†’ <code>/public/screenshots/improvement-plan.png</code></div>
</div>

AI-en lager utbedringsplaner:
- For enkeltpunkter
- Samlet plan med prioritert rekkefÃ¸lge
- Estimert kostnad og tidsbruk

</v-click>

<!--
Funksjon tre: Varsler.
Gjensidige sender varsler til boligeiere â€” frost, kraftig nedbÃ¸r, stormvarsel.
Brukeren kan klikke pÃ¥ et varsel og fÃ¥ personaliserte tiltak for sin bolig.

Funksjon fire: Forbedringsplaner.
AI-en lager utbedringsplaner for enkeltpunkter eller en samlet plan med prioritert rekkefÃ¸lge.
-->

---

# Teknisk arkitektur â€” helt overordnet

```mermaid {scale: 0.75}
graph LR
    subgraph Frontend
        A[React Router Frontend]
    end
    
    subgraph Backend["FastAPI Backend"]
        B[Guardrails]
        C[RAG / pgvector]
        D[Conversation Memory]
    end
    
    subgraph Infra["Infrastructure"]
        E[PostgreSQL + pgvector]
        F[Azure OpenAI]
        G[MLflow]
    end
    
    A --> Backend
    Backend --> Infra
```

<v-click>

**Tech-stacken:**
- Python, LangGraph + LangChain for agentorkestrering
- Azure OpenAI (via Gjensidiges interne endpoint)
- PostgreSQL med pgvector for data og RAG
- **MLflow for observabilitet** â† Dette er fokuset i dag

</v-click>

<!--
Her er den tekniske arkitekturen pÃ¥ hÃ¸yt nivÃ¥.

Vi har en React Router frontend med Gjensidige Builders design system.
FastAPI backend med guardrails for validering, RAG med pgvector, og conversation memory i PostgreSQL.
Infrastrukturen bestÃ¥r av PostgreSQL med pgvector, Azure OpenAI via Gjensidiges interne endpoint, og MLflow for observabilitet.

Tech-stacken er: Python, LangGraph + LangChain for agentorkestrering, Azure OpenAI, PostgreSQL med pgvector, og â€” det vi skal fokusere pÃ¥ i dag â€” MLflow for observabilitet.
-->

---
layout: section
class: text-center
---

## Observabilitet og sporbarhet

<!--
NÃ¥ som dere forstÃ¥r hva vi bygger, la oss snakke om hvorfor og hvordan vi legger til observabilitet.
Dette er kjernen av presentasjonen.
-->

---

# Hvorfor er observabilitet viktig?

Vi bygger en **MLS** â€” en "minimum lovable service" for pilotbrukerne vÃ¥re.

Det betyr at vi _mÃ¥_ vite:

<v-clicks>

<div class="mt-8 grid grid-cols-3 gap-4">

<div class="border rounded p-4">
  <div class="text-3xl mb-2">ğŸ‘¥</div>
  <div class="font-bold mb-2">Brukeratferd</div>
  <div class="text-sm">Hvordan bruker folk AI-delene? Stiller de spÃ¸rsmÃ¥l vi ikke forventet?</div>
</div>

<div class="border rounded p-4">
  <div class="text-3xl mb-2">ğŸ¤–</div>
  <div class="font-bold mb-2">AgentoppfÃ¸rsel</div>
  <div class="text-sm">Klassifiserer de intensjon riktig? Hallusinerer de?</div>
</div>

<div class="border rounded p-4">
  <div class="text-3xl mb-2">ğŸ“„</div>
  <div class="font-bold mb-2">Ekstraksjonskvalitet</div>
  <div class="text-sm">Finner AI-en alle tilstandsvurderinger i dokumentene?</div>
</div>

</div>

</v-clicks>

<v-click>

<div class="mt-8 p-4 bg-yellow-500 bg-opacity-10 rounded">

**Personlig tenker jeg pÃ¥ observabilitet som en mÃ¥te Ã¥ introdusere _litt determinisme i det udeterministiske_ â€” eller i det minste litt klarhet der ting ikke alltid er like tydelig.**

</div>

</v-click>

<!--
Vi bygger en MLS â€” en "minimum lovable service" for pilotbrukerne vÃ¥re. Det betyr at vi mÃ¥ vite:

1. Hvordan bruker folk AI-delene? Stiller de spÃ¸rsmÃ¥l vi ikke forventet? Treffer RAG-modulen riktig kontekst?
2. Hvordan oppfÃ¸rer agentene seg? Klassifiserer de intensjon riktig? Hallusinerer de?
3. Fungerer ekstraheringen? Finner AI-en alle tilstandsvurderinger i dokumentene?

Personlig tenker jeg pÃ¥ observabilitet som en mÃ¥te Ã¥ introdusere litt determinisme i det udeterministiske â€” eller i det minste litt klarhet der ting ikke alltid er like tydelig.
-->

---

# MLflow UI â€” Traces-oversikten

<div class="border-2 border-dashed border-yellow-400 rounded-lg p-8 text-yellow-400 text-center mt-8 h-80 flex flex-col items-center justify-center">
  <div class="text-4xl mb-4">ğŸ“Š</div>
  <div class="text-sm font-bold">ğŸ“¸ PLACEHOLDER: MLflow UI â€” Traces-oversikten</div>
  <div class="text-xs mt-2">En liste med traces, med kolonner for trace ID, status, latency, og tidspunkt</div>
  <div class="text-xs opacity-75 mt-1">Place as <code>/public/screenshots/mlflow-traces-overview.png</code></div>
</div>

<!--
Her ser dere MLflow UI â€” traces-oversikten.
Dette er en liste med alle traces, med kolonner for trace ID, status, latency, og tidspunkt.
-->

---

# Slik setter vi opp MLflow-tracing

Alt begynner med Ã©n funksjon: `init_mlflow()`

````md magic-move
```python
# api/src/observability/tracing.py

import mlflow
from src.config import get_settings

def init_mlflow() -> None:
    """Initialize MLflow tracking."""
    settings = get_settings()
```

```python
# api/src/observability/tracing.py

import mlflow
from src.config import get_settings

def init_mlflow() -> None:
    """Initialize MLflow tracking."""
    settings = get_settings()

    # Sett tracking URI â€” peker pÃ¥ MLflow-serveren
    mlflow.set_tracking_uri(settings.mlflow_tracking_uri)

    # Sett eksperiment â€” alle traces havner her
    mlflow.set_experiment(settings.mlflow_experiment_name)
```

```python {16-17}
# api/src/observability/tracing.py

import mlflow
from src.config import get_settings

def init_mlflow() -> None:
    """Initialize MLflow tracking."""
    settings = get_settings()

    # Sett tracking URI â€” peker pÃ¥ MLflow-serveren
    mlflow.set_tracking_uri(settings.mlflow_tracking_uri)

    # Sett eksperiment â€” alle traces havner her
    mlflow.set_experiment(settings.mlflow_experiment_name)

    # Ã‰n linje â€” dette er magien
    mlflow.langchain.autolog(log_traces=True)
```
````

<!--
Alt begynner med Ã©n funksjon: init_mlflow(). Her er koden:

FÃ¸rst setter vi tracking URI â€” dette peker pÃ¥ MLflow-serveren.
SÃ¥ setter vi eksperiment â€” alle traces havner her.

Og sÃ¥ kommer magien: mlflow.langchain.autolog(log_traces=True)

Den siste linjen er nÃ¸kkelen. Denne ene linjen gjÃ¸r at alle LangChain- og LangGraph-kall automatisk blir tracet.
Hver node i grafen, hvert LLM-kall, hvert verktÃ¸ykall â€” alt logges til MLflow uten at vi skriver en eneste dekoratÃ¸r eller wrapper.

Denne funksjonen kalles nÃ¥r FastAPI-serveren starter opp, i main.py.
Vi kjÃ¸rer med 100% sampling i MVP-fasen â€” hvert eneste brukerbudskap blir tracet.
-->

---

# Hva tracer vi â€” og hva ser vi?

## Chat-agenten: En LangGraph-graf

```mermaid {scale: 0.6}
graph LR
    Start(("ğŸŸ¢")) --> validate_input
    validate_input -->|avvist| End(("ğŸ”´"))
    validate_input -->|ok| classify_intent
    classify_intent -->|trenger kontekst| retrieve_context
    classify_intent -->|direkte svar| generate_response
    retrieve_context --> generate_response
    generate_response --> End
```

<v-click>

Takket vÃ¦re `mlflow.langchain.autolog()` kan vi i MLflow UI se **hvert steg** som en span i tracet.

</v-click>

<!--
Chat-agenten vÃ¥r er definert som en LangGraph StateGraph. Slik ser den ut:

Start â†’ validate_input â†’ classify_intent â†’ retrieve_context (hvis nÃ¸dvendig) â†’ generate_response â†’ End

Takket vÃ¦re mlflow.langchain.autolog() kan vi i MLflow UI se hvert steg som en span i tracet.
-->

---

# MLflow UI â€” Utvidet trace

<div class="border-2 border-dashed border-yellow-400 rounded-lg p-8 text-yellow-400 text-center mt-8 h-80 flex flex-col items-center justify-center">
  <div class="text-4xl mb-4">ğŸ”</div>
  <div class="text-sm font-bold">ğŸ“¸ PLACEHOLDER: MLflow UI â€” Utvidet trace</div>
  <div class="text-xs mt-2">Span-treet med validate_input â†’ classify_intent â†’ retrieve_context â†’ generate_response</div>
  <div class="text-xs mt-1">med input/output for hver span og latency-bar</div>
  <div class="text-xs opacity-75 mt-1">Place as <code>/public/screenshots/mlflow-trace-expanded.png</code></div>
</div>

<!--
Her ser dere et utvidet trace i MLflow UI â€” span-treet som viser:
- validate_input â€” Ble meldingen avvist av guardrails?
- classify_intent â€” Hva ble intensjonen?
- retrieve_context â€” Hvilke dokumentchunks ble hentet fra vektordatabasen?
- generate_response â€” Hva var den fulle prompten, og hva svarte LLM-en?

For hver span ser vi input, output, og latency. Uten Ã¥ skrive en eneste linje observabilitetskode utover autolog().
-->

---

# Agenttilstanden â€” hva som flyter gjennom grafen

Tilstanden som flyter mellom nodene er definert som en TypedDict:

```python
# api/src/agent/state.py

class AgentState(TypedDict, total=False):
    messages: Annotated[list[BaseMessage], add_messages]
    conversation_id: uuid.UUID | None
    resolved_context: dict | None        # Eiendoms-/tilstandskontekst
    retrieved_docs: list[Document]       # RAG-resultater
    sources: list[dict]                  # Kildemetadata for siteringer
    current_step: str                    # Observabilitet
    intent: str | None                   # Klassifisert intensjon
    needs_retrieval: bool                # Trenger vi RAG?
    is_rejected: bool                    # Avvist av guardrails?
```

<v-click>

<div class="mt-4 p-4 bg-blue-500 bg-opacity-10 rounded">

Alt dette er synlig i hvert trace â€” vi kan se nÃ¸yaktig hva grafen "tenkte" i hvert steg.

</div>

</v-click>

<!--
Tilstanden som flyter mellom nodene er definert som en TypedDict.

Her ser dere alle feltene â€” messages, conversation_id, resolved_context, retrieved_docs, sources, current_step, intent, needs_retrieval, is_rejected.

Alt dette er synlig i hvert trace â€” vi kan se nÃ¸yaktig hva grafen "tenkte" i hvert steg.
-->

---

# Ekstraheringsagenten: Deep Agents med subagenter

<div class="text-sm">

For dokumentanalyse bruker vi **Deep Agents** â€” et abstraksjonslag over LangChain og LangGraph.

</div>

<v-click>

```python
from deepagents import create_deep_agent

def create_extraction_agent(...) -> Runnable:
    model = get_extraction_model()
    tools = [...]           # verktÃ¸y for parsing, lagring, fremdrift
    subagent = [...]        # subagent for ekstraksjon av tilstandsvurderinger

    return create_deep_agent(
        model=model,
        tools=tools,
        subagents=subagent,
        system_prompt=ORCHESTRATOR_SYSTEM_PROMPT,
    )
```

</v-click>

<v-click>

<div class="mt-2 p-3 bg-purple-500 bg-opacity-10 rounded text-sm">

Fordi Deep Agents bygger pÃ¥ LangChain, fÃ¥r vi **automatisk tracing av hele orkestreringsagenten og alle subagent-kall** via den samme `autolog()`-linjen.

</div>

</v-click>

<!--
For dokumentanalyse bruker vi Deep Agents-biblioteket â€” et bibliotek for Ã¥ bygge agenter med planlegging, subagenter og verktÃ¸y, bygget av LangChain-teamet.
Tenk pÃ¥ det som et abstraksjonslag over LangChain og LangGraph for Ã¥ fÃ¥ kjernen av Claude Code out-of-the-box uten Ã¥ mÃ¥tte skrive all kode selv.

Fordi Deep Agents bygger pÃ¥ LangChain, fÃ¥r vi automatisk tracing av hele orkestreringsagenten og alle subagent-kall via den samme autolog()-linjen.
-->

---

# MLflow UI â€” Ekstraheringsagent trace

<div class="border-2 border-dashed border-yellow-400 rounded-lg p-8 text-yellow-400 text-center mt-8 h-80 flex flex-col items-center justify-center">
  <div class="text-4xl mb-4">ğŸŒ³</div>
  <div class="text-sm font-bold">ğŸ“¸ PLACEHOLDER: MLflow UI â€” Ekstraheringsagent trace</div>
  <div class="text-xs mt-2">Dypt span-tre med orkestrator-agenten Ã¸verst, condition-extractor-subagenten nÃ¸stet under,</div>
  <div class="text-xs">og individuelle LLM-kall med tool calls synlige</div>
  <div class="text-xs opacity-75 mt-1">Place as <code>/public/screenshots/mlflow-extraction-trace.png</code></div>
</div>

<!--
Her ser dere et ekstraheringsagent-trace i MLflow UI â€” det dype span-treet med orkestrator-agenten Ã¸verst, condition-extractor-subagenten nÃ¸stet under, og individuelle LLM-kall med tool calls synlige.

I MLflow UI kan vi se:
- Hovedagentens resonnering og verktÃ¸ykall
- Subagentens ekstraheringsresultater
- Hvert LLM-kall med full prompt og respons
- Tidsbruk per fase
-->

---

# Evaluering â€” Custom scorers

Tracing gir oss innsikt i _hva_ som skjer. Men vi trenger ogsÃ¥ Ã¥ mÃ¥le _hvor bra_ det fungerer.

Vi har bygget fire tilpassede scorers med **MLflow GenAI Evaluation**:

<v-clicks>

<div class="grid grid-cols-2 gap-4 mt-4">

<div class="border rounded p-3">
  <div class="text-xl mb-1">ğŸ¯</div>
  <div class="font-bold text-sm">GroundednessScorer</div>
  <div class="text-xs opacity-75">Er svaret forankret i konteksten? Fanger opp hallusinasjoner.</div>
</div>

<div class="border rounded p-3">
  <div class="text-xl mb-1">ğŸ’¡</div>
  <div class="font-bold text-sm">HelpfulnessScorer</div>
  <div class="text-xs opacity-75">Adresserer svaret brukerens spÃ¸rsmÃ¥l?</div>
</div>

<div class="border rounded p-3">
  <div class="text-xl mb-1">ğŸ“š</div>
  <div class="font-bold text-sm">RetrievalRelevanceScorer</div>
  <div class="text-xs opacity-75">Er den hentede konteksten relevant for spÃ¸rsmÃ¥let?</div>
</div>

<div class="border rounded p-3">
  <div class="text-xl mb-1">ğŸ‡³ğŸ‡´</div>
  <div class="font-bold text-sm">NorwegianLanguageScorer</div>
  <div class="text-xs opacity-75">Svarer agenten pÃ¥ norsk? (Viktig for oss!)</div>
</div>

</div>

</v-clicks>

<!--
Tracing gir oss innsikt i hva som skjer.
Men vi trenger ogsÃ¥ Ã¥ mÃ¥le hvor bra det fungerer.

Her bruker vi MLflow GenAI Evaluation.

Vi har bygget fire tilpassede scorers:

1. GroundednessScorer â€” Er svaret forankret i konteksten? Fanger opp hallusinasjoner.
2. HelpfulnessScorer â€” Adresserer svaret brukerens spÃ¸rsmÃ¥l? En hÃ¸flig avvisning av off-topic-spÃ¸rsmÃ¥l regnes som "helpful".
3. RetrievalRelevanceScorer â€” Er den hentede konteksten relevant for spÃ¸rsmÃ¥let?
4. NorwegianLanguageScorer â€” Svarer agenten pÃ¥ norsk? Viktig for oss!
-->

---

# GroundednessScorer â€” Kodeeksempel

```python {*|4-6|7-8|10|12-16}{lines:true}
from mlflow.genai import scorer
from mlflow.entities import Feedback

def GroundednessScorer():
    @scorer
    def groundedness(*, inputs, outputs, context=None, **kwargs):
        if intent in ["REJECTED", "GENERAL_CHAT"]:
            return None                    # hopp over irrelevante intensjoner

        result = _groundedness_judge(...)  # LLM-dommer vurderer forankring

        return Feedback(
            value=...,      # 1.0 = forankret, 0.5 = delvis, 0.0 = hallusinert
            rationale=...,  # forklaring fra dommeren
        )
    return groundedness
```

<!--
Her er koden for GroundednessScorer.

Vi bruker @scorer-dekoratÃ¸ren fra MLflow.
FÃ¸rst hopper vi over evaluering for REJECTED og GENERAL_CHAT-intensjoner â€” det gir ikke mening Ã¥ evaluere forankring for "Hei, hvordan har du det?".
SÃ¥ kaller vi en strukturert output-dommer som vurderer om svaret er forankret i konteksten.
Vi mapper resultatet til en score: 1.0 for fully grounded, 0.5 for partially grounded, 0.0 for hallucinated.
Og vi returnerer Feedback med score og rationale.

Legg merke til at scorerne er intensjonsbevisste. Hvis intensjonen er GENERAL_CHAT, hopper vi over groundedness-evalueringen.
-->

---

# KjÃ¸re evalueringer â€” CLI

Vi har en CLI for Ã¥ kjÃ¸re evalueringer mot utvalgte testdatasett eller produksjons-traces:

```bash
# Evaluer mot utvalgt testdatasett (17 testtilfeller)
uv run --directory api python -m src.evaluation.cli conversation
```

<v-click>

```bash
# Evaluer de siste 100 produksjons-tracene
uv run --directory api python -m src.evaluation.cli conversation \
    --from-traces --trace-count 100
```

</v-click>

<v-click>

```bash
# Evaluer med 20% sampling fra siste uke
uv run --directory api python -m src.evaluation.cli conversation \
    --from-traces --start-date 2026-01-01 --sample-rate 0.2
```

</v-click>

<!--
Vi har en CLI for Ã¥ kjÃ¸re evalueringer.

FÃ¸rste kommando: Evaluer mot utvalgt testdatasett med 17 testtilfeller.
Andre kommando: Evaluer de siste 100 produksjons-tracene.
Tredje kommando: Evaluer med 20% sampling fra siste uke.
-->

---

# MLflow UI â€” Evaluation results

<div class="border-2 border-dashed border-yellow-400 rounded-lg p-8 text-yellow-400 text-center mt-8 h-80 flex flex-col items-center justify-center">
  <div class="text-4xl mb-4">ğŸ“ˆ</div>
  <div class="text-sm font-bold">ğŸ“¸ PLACEHOLDER: MLflow UI â€” Evaluation results</div>
  <div class="text-xs mt-2">Tabell med aggregerte metrikker (pass rate, gjennomsnittsscore per scorer)</div>
  <div class="text-xs">og rader med individuelle testtilfeller som viser score + rationale</div>
  <div class="text-xs opacity-75 mt-1">Place as <code>/public/screenshots/mlflow-evaluation-results.png</code></div>
</div>

<!--
Resultatene havner i MLflow UI med:
- Aggregerte metrikker â€” pass rate, gjennomsnittsscore per scorer
- Individuelle testresultater med rationale
- Koblet til traces â€” du kan klikke deg ned i nÃ¸yaktig hva agenten gjorde
-->

---

# Utvalgte testdatasett

Vi har definert testtilfeller som dekker ulike scenarioer:

```python
# api/src/evaluation/datasets/conversation.py

PROPERTY_TEST_CASES = [
    ConversationTestCase(
        id="prop_001",
        question="Hvor gammelt er taket mitt?",
        expected_intent="PROPERTY_QUESTION",
        context="ByggeÃ¥r: 1985. Taket ble lagt om i 2010.",
        reference_answer="Taket ble lagt om i 2010, ca. 15 Ã¥r gammelt.",
    ),
]

OUT_OF_SCOPE_TEST_CASES = [
    ConversationTestCase(
        id="oos_001",
        question="Kan du hjelpe meg med matlagning?",
        expected_intent="REJECTED",
    ),
]
```

<v-click>

<div class="mt-2 p-3 bg-blue-500 bg-opacity-10 rounded text-sm">

Disse testtilfellene kan kjÃ¸res som **regresjonstester** â€” etter hver endring i prompts eller agentkode kan vi verifisere at kvaliteten holder seg.

</div>

</v-click>

<!--
Vi har definert testtilfeller som dekker ulike scenarioer.

Her er et eksempel pÃ¥ en PROPERTY_TEST_CASE â€” "Hvor gammelt er taket mitt?" med forventet intensjon og referansesvar.
Og et eksempel pÃ¥ en OUT_OF_SCOPE_TEST_CASE â€” "Kan du hjelpe meg med matlagning?" som skal avvises.

Disse testtilfellene kan kjÃ¸res som regresjonstester â€” etter hver endring i prompts eller agentkode kan vi verifisere at kvaliteten holder seg.
-->

---

# Produksjons-traces â†’ Evaluering

<div class="text-sm">

Det virkelig kraftige er at vi kan ta _ekte_ brukerinteraksjoner fra produksjon og evaluere dem:

</div>

```python {*|1-10|12-20}{lines:true}
# api/src/evaluation/datasets/traces.py

def query_mlflow_traces(experiment_name, trace_count, start_date):
    """Hent traces fra MLflow for evaluering."""
    client = mlflow.MlflowClient()
    return client.search_traces(
        experiment_ids=[experiment.experiment_id],
        max_results=trace_count,
        order_by=["timestamp_ms DESC"],
    )

def convert_traces_to_dataset(traces):
    """Konverter MLflow-traces til evalueringsdatasett."""
    dataset = []
    for trace in traces:
        dataset.append({
            "inputs": {"question": question},
            "outputs": {"answer": answer, "context": context, "intent": intent},
        })
    return dataset
```

<!--
Det virkelig kraftige er at vi kan ta ekte brukerinteraksjoner fra produksjon og evaluere dem.

Vi har en funksjon som henter traces fra MLflow for evaluering.
Og en funksjon som konverterer MLflow-traces til evalueringsdatasett.

Vi ekstraher spÃ¸rsmÃ¥l, svar, kontekst og intensjon fra tracens request/response data.
-->

---

# Produksjons-traces â†’ Evaluering â€” Flyt

```mermaid {scale: 0.8}
graph LR
    A[Produksjons-traces<br/>MLflow] --> B[Filtrer/sample]
    B --> C[Evalueringsdatasett]
    C --> D[Scorers]
    D --> E[Resultater<br/>tilbake i MLflow UI]
```

<!--
Flyten er: Produksjon â†’ MLflow traces â†’ Filtrer/sample â†’ Evalueringsdatasett â†’ Scorers â†’ Resultater tilbake i MLflow UI.

Dette gir oss muligheten til Ã¥ kontinuerlig evaluere kvaliteten pÃ¥ produksjons-interaksjoner.
-->

---
layout: section
class: text-center
---

## Utvikling og hosting

<!--
La oss se pÃ¥ hvordan vi kjÃ¸rer dette lokalt og i produksjon.
-->

---

# Lokal utvikling â€” Docker Compose

<div class="text-sm">

For lokal utvikling er MLflow del av vÃ¥r Docker Compose:

</div>

```yaml {*|7-17}{lines:true}
# docker-compose.yml
services:
  postgres:
    image: pgvector/pgvector:pg16
    ports: ["5432:5432"]

  mlflow:
    build:
      dockerfile: docker/mlflow.Dockerfile
    command: >
      mlflow server
      --host 0.0.0.0 --port 5000
      --backend-store-uri postgresql://...@postgres:5432/mlflow
      --serve-artifacts
    ports: ["5000:5000"]
    depends_on:
      postgres: { condition: service_healthy }
```

<!--
For lokal utvikling er MLflow del av vÃ¥r Docker Compose.

Vi har PostgreSQL med pgvector, MLflow som kjÃ¸rer som en egen container med PostgreSQL som backend-store, og Redis.

make db-up starter alt. http://localhost:5000 gir deg MLflow UI.
-->

---

# Produksjon â€” Databricks

For produksjon vil vi hoste MLflow i **Databricks** innenfor Gjensidige sin skyrigg pÃ¥ Azure.

<v-clicks>

<div class="mt-8">

**Hvorfor Databricks?**
- MLflow har native integrasjon med Databricks
- Gjensidige bruker allerede Databricks for dataplattformen sin
- Skalerer automatisk
- Managed service â€” ingen infrastruktur Ã¥ vedlikeholde

</div>

<div class="mt-8">

**Alternativer for produksjonsskala (fra MLflows dokumentasjon):**
- AWS SageMaker
- Azure Machine Learning
- Nebius
- GCP (GKE)

</div>

</v-clicks>

<!--
For produksjon vil vi hoste MLflow i Databricks innenfor Gjensidige sin skyrigg pÃ¥ Azure.

Hvorfor Databricks?
- MLflow har native integrasjon med Databricks
- Gjensidige bruker allerede Databricks for dataplattformen sin
- Skalerer automatisk
- Managed service â€” ingen infrastruktur Ã¥ vedlikeholde

Fra MLflows egen dokumentasjon â€” alternativer for produksjonsskala:
AWS SageMaker, Azure Machine Learning, Nebius, GCP.
Vi bruker Databricks.
-->

---
layout: section
class: text-center
---

## Oppsummering og veien videre

<!--
La oss oppsummere det vi har gÃ¥tt gjennom.
-->

---

# Den store idÃ©en

<v-click>

<div class="text-2xl text-center mt-12 mb-12 p-8 bg-purple-500 bg-opacity-10 rounded">

Ved Ã¥ dumpe all denne innsikten â€” hvert agentresonnement, hvert RAG-oppslag, hvert evalueringsresultat â€” inn i MLflow, kan vi gjÃ¸re **dataanalyse pÃ¥ bruker- og agentatferd**.

</div>

</v-click>

<v-click>

<div class="text-2xl text-center mt-12 p-8 bg-blue-500 bg-opacity-10 rounded">

Vi kan tweake systemet vÃ¥rt, og skape en brukeropplevelse som brukerne faktisk _elsker_.

</div>

</v-click>

<!--
Poenget med alt dette er:

Ved Ã¥ dumpe all denne innsikten â€” hvert agentresonnement, hvert RAG-oppslag, hvert evalueringsresultat â€” inn i MLflow, kan vi gjÃ¸re dataanalyse pÃ¥ bruker- og agentatferd.

Vi kan tweake systemet vÃ¥rt, og skape en brukeropplevelse som brukerne faktisk elsker.
-->

---

# Feedback-loopen

<div class="border-2 border-dashed border-yellow-400 rounded-lg p-3 text-yellow-400 text-center mt-2 h-28 flex flex-col items-center justify-center">
  <div class="text-2xl mb-1">ğŸ‘ğŸ‘</div>
  <div class="text-xs font-bold">ğŸ“¸ PLACEHOLDER: Chat UI med feedback-knapper</div>
  <div class="text-xs mt-1">Tommel opp/ned-knapper under et AI-svar</div>
  <div class="text-xs opacity-75">â†’ <code>/public/screenshots/chat-feedback-ui.png</code></div>
</div>

<v-click>

<div class="text-sm mt-2">

Par dette med et **tilbakemeldingssystem** fra brukerne â€” og vi har en transparent mÃ¥te Ã¥ korrigere agentenes oppfÃ¸rsel i produksjon.

</div>

</v-click>

<v-click>

```mermaid {scale: 0.55}
graph LR
    A[Bruker] --> B[Feedback ğŸ‘/ğŸ‘]
    B --> C[Database]
    C --> D[MLflow Assessment]
    D --> E[Evaluering]
    E --> F[Prompt-forbedring]
    F --> G[Bedre svar]
    G --> A
```

</v-click>

<!--
Par dette med et tilbakemeldingssystem fra brukerne â€” tommel opp/ned og kommentarer i chatten, eller flagging av at en ekstraksjon er helt feil â€” og vi har en transparent mÃ¥te Ã¥ korrigere agentenes oppfÃ¸rsel i produksjon.

Denne sirkelen â€” fra brukerinteraksjon til observabilitet til evaluering til forbedring â€” er det som gjÃ¸r det mulig Ã¥ skalere agentiske lÃ¸sninger med selvtillit.
-->

---

# Oppsummert

<div class="text-sm">

| Hva                     | Hvordan                                    | VerktÃ¸y            |
| ----------------------- | ------------------------------------------ | ------------------ |
| Automatisk tracing      | `mlflow.langchain.autolog()`               | MLflow + LangChain |
| Agent-orkestrering      | LangGraph StateGraph                       | LangGraph          |
| Dokumentekstraksjon     | Deep Agents med subagenter                 | `deepagents`       |
| KvalitetsmÃ¥ling         | Custom scorers + `mlflow.genai.evaluate()` | MLflow GenAI       |
| Produksjonsmonitorering | Trace-basert evaluering med sampling       | MLflow traces API  |
| Bruker-feedback         | Thumbs up/down â†’ MLflow assessments        | FastAPI + MLflow   |

</div>

<v-click>

<div class="mt-4 p-3 bg-green-500 bg-opacity-10 rounded text-center text-sm">

**Ã‰n linje kode gir oss full observabilitet.** Et sett med scorers gir oss systematisk kvalitetsmÃ¥ling. Og muligheten til Ã¥ evaluere produksjons-traces gir oss den tryggheten vi trenger for Ã¥ skalere.

</div>

</v-click>

<!--
La meg oppsummere:

Automatisk tracing med mlflow.langchain.autolog() â€” MLflow + LangChain
Agent-orkestrering med LangGraph StateGraph â€” LangGraph
Dokumentekstraksjon med Deep Agents med subagenter â€” deepagents
KvalitetsmÃ¥ling med custom scorers og mlflow.genai.evaluate() â€” MLflow GenAI
Produksjonsmonitorering med trace-basert evaluering med sampling â€” MLflow traces API
Bruker-feedback med thumbs up/down til MLflow assessments â€” FastAPI + MLflow

Ã‰n linje kode gir oss full observabilitet.
Et sett med scorers gir oss systematisk kvalitetsmÃ¥ling.
Og muligheten til Ã¥ evaluere produksjons-traces gir oss den tryggheten vi trenger for Ã¥ skalere.
-->

---
layout: center
class: text-center
---

# Takk for oppmerksomheten!

## SpÃ¸rsmÃ¥l?

<div class="mt-12">

**Magnus RÃ¸dseth og HÃ¥vard Opheim**

Capra Consulting @ Gjensidige

</div>

<div class="abs-br m-6 flex gap-8 items-center bg-white rounded-lg px-4 py-2">
  <img src="/capra-logo.png" class="h-5" alt="Capra Consulting" />
  <img src="/gjensidige-logo.png" class="h-6" alt="Gjensidige" />
</div>

<!--
Takk for oppmerksomheten! SpÃ¸rsmÃ¥l?
-->
